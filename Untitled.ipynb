{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9924eb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    " \n",
    "def read_and_preprocess(lang, sample_fraction=0.5):\n",
    "    dataset_path = f'./datasets/CL_{lang}-en.parquet'\n",
    "    df_with_en = pd.read_parquet(dataset_path)\n",
    "    df_lang = df_with_en[[lang]].rename(columns={lang: 'text'})\n",
    "    df_lang['language'] = lang\n",
    "    # Randomly sample a fraction of the data\n",
    "    df_lang = df_lang.sample(frac=sample_fraction, random_state=42)  # You can set a random seed for reproducibility\n",
    "\n",
    "    df_lang = df_lang.dropna(subset=['text'])\n",
    "    \n",
    "#     df_lang['text'] = df_lang['text'].apply(lambda x: ' '.join(x.lower().split()))\n",
    "        # Explode paragraphs into separate words using whitespace-based tokenization\n",
    "    df_lang['text'] = df_lang['text'].apply(lambda x: x.lower().split())\n",
    "\n",
    "    return df_lang \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b5800fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context_target_pairs(words, context_window_size=2):\n",
    "    pairs = []\n",
    "    \n",
    "    for i, target_word in enumerate(words):\n",
    "        # Define a context window around the target word\n",
    "        start = max(0, i - context_window_size)\n",
    "        end = min(len(words), i + context_window_size + 1)\n",
    "        \n",
    "        # Extract context words within the window\n",
    "        context_words = [words[j] for j in range(start, end) if j != i]\n",
    "        \n",
    "        # Create context-target pairs\n",
    "        for context_word in context_words:\n",
    "            pairs.append((target_word, context_word))\n",
    "    \n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c67667f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing frech\n",
      "processed french\n",
      "getting al words\n",
      "got all words\n",
      "chunks ready\n",
      "chunk done\n",
      "chunk done\n",
      "chunk done\n",
      "chunk done\n",
      "chunk done\n",
      "chunk done\n",
      "chunk done\n",
      "chunk done\n",
      "chunk done\n",
      "chunk done\n",
      "[('très', 'peu'), ('très', 'de'), ('peu', 'très'), ('peu', 'de'), ('peu', 'facteurs'), ('de', 'très'), ('de', 'peu'), ('de', 'facteurs'), ('de', 'ont'), ('facteurs', 'peu')]\n"
     ]
    }
   ],
   "source": [
    "print(\"processing frech\")\n",
    "df_fr = read_and_preprocess('fr')\n",
    "print(\"processed french\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(\"getting al words\")\n",
    "\n",
    "all_words_fr = df_fr['text'].sum()  # Combine all words into a single list\n",
    "\n",
    "print(\"got all words\")\n",
    "\n",
    "# Split the list of words into smaller chunks\n",
    "chunk_size = 1000\n",
    "chunks = [all_words_fr[i:i+chunk_size] for i in range(0, 10000, chunk_size)]\n",
    "\n",
    "print(\"chunks ready\")\n",
    "context_target_pairs_fr = []\n",
    "\n",
    "# Process each chunk and create pairs\n",
    "for chunk in chunks:\n",
    "    pairs = create_context_target_pairs(chunk, context_window_size=2)\n",
    "    context_target_pairs_fr.extend(pairs)\n",
    "    print(\"chunk done\")\n",
    "\n",
    "# Display a few pairs as an example\n",
    "print(context_target_pairs_fr[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3073f80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, target):\n",
    "        embedded = self.embeddings(target)\n",
    "        output = self.linear(embedded)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e64b44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Build a vocabulary and word-to-index mapping\n",
    "vocab = set(all_words_fr)  # Assuming 'all_words_fr' contains all unique words\n",
    "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "# Define hyperparameters\n",
    "embedding_dim = 100\n",
    "vocab_size = len(vocab)  # Replace with the actual vocabulary size\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Create the Word2Vec model\n",
    "model = Word2Vec(vocab_size, embedding_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     total_loss = 0.0\n",
    "    \n",
    "#     for target, context in context_target_pairs_fr:\n",
    "#         # Convert target and context words to indices\n",
    "#         target_index = word_to_index[target]\n",
    "#         context_index = word_to_index[context]\n",
    "        \n",
    "#         # Zero the gradients\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         # Forward pass\n",
    "#         output = model(torch.tensor([target_index], dtype=torch.long))\n",
    "        \n",
    "#         # Calculate loss\n",
    "#         loss = criterion(output, torch.tensor([context_index], dtype=torch.long))\n",
    "        \n",
    "#         # Backpropagation\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         total_loss += loss.item()\n",
    "    \n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6997ea40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Avg. Loss: 9.906805409240723\n",
      "Epoch 2/10, Avg. Loss: 8.172392933654786\n",
      "Epoch 3/10, Avg. Loss: 7.212105236816406\n",
      "Epoch 4/10, Avg. Loss: 6.5407790786743165\n",
      "Epoch 5/10, Avg. Loss: 6.056113903808594\n",
      "Epoch 6/10, Avg. Loss: 5.6935494606018064\n",
      "Epoch 7/10, Avg. Loss: 5.4135796133041385\n",
      "Epoch 8/10, Avg. Loss: 5.191718505096436\n",
      "Epoch 9/10, Avg. Loss: 5.012246129989624\n",
      "Epoch 10/10, Avg. Loss: 4.864613343048096\n"
     ]
    }
   ],
   "source": [
    "# Define batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    batch_losses = []\n",
    "\n",
    "    for i in range(0, len(context_target_pairs_fr), batch_size):\n",
    "        batch = context_target_pairs_fr[i:i+batch_size]\n",
    "        \n",
    "        # Prepare inputs and targets for the batch\n",
    "        targets, contexts = zip(*batch)\n",
    "        target_indices = torch.tensor([word_to_index[target] for target in targets], dtype=torch.long)\n",
    "        context_indices = torch.tensor([word_to_index[context] for context in contexts], dtype=torch.long)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(target_indices)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, context_indices)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        batch_losses.append(loss.item())\n",
    "    \n",
    "    # Print average loss for the epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Avg. Loss: {total_loss / len(batch_losses)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8ad79481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Assuming you have a list of words in your vocabulary\n",
    "your_vocab = all_words_fr\n",
    "\n",
    "# Retrieve word vectors for specific words\n",
    "word_vectors_dict = {}\n",
    "for word in your_vocab:\n",
    "    # Convert the word to its corresponding index in the vocabulary\n",
    "    word_index = torch.tensor([word_to_index[word]], dtype=torch.long)\n",
    "    \n",
    "    # Pass the word index through your Word2Vec model to get the word vector\n",
    "    word_vector = model.embeddings(word_index)\n",
    "    \n",
    "    # Convert the word vector to a numpy array\n",
    "    word_vector = word_vector.squeeze().detach().numpy()\n",
    "    \n",
    "    # Store the word vector in a dictionary\n",
    "    word_vectors_dict[word] = word_vector\n",
    "\n",
    "# # Perform vector arithmetic\n",
    "# roi_vector = word_vectors_dict['roi']\n",
    "# homme_vector = word_vectors_dict['homme']\n",
    "# femme_vector = word_vectors_dict['femme']\n",
    "# result_vector = roi_vector - homme_vector + femme_vector\n",
    "\n",
    "\n",
    "# Calculate cosine similarity between a vector and a list of vectors while excluding input words\n",
    "def find_closest_words_excluding_input(vector, vectors_dict, input_words, num_results=5):\n",
    "    # Convert the input vector to a PyTorch tensor (if it's not already)\n",
    "    vector = torch.tensor(vector, dtype=torch.float32)\n",
    "\n",
    "    # Calculate cosine similarities between the input vector and all vectors in the dictionary\n",
    "    similarities = {word: F.cosine_similarity(vector, torch.tensor(vec), dim=0).item()\n",
    "                    for word, vec in vectors_dict.items()}\n",
    "\n",
    "    # Exclude the input words from the closest words list\n",
    "    closest_words = [word for word in sorted(similarities, key=similarities.get, reverse=True)\n",
    "                     if word not in input_words]\n",
    "\n",
    "    # Take the top 'num_results' closest words (excluding input words)\n",
    "    closest_words = closest_words[:num_results]\n",
    "\n",
    "    return closest_words\n",
    "\n",
    "# ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "186a61d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: [\"d'altitude\", 'dire.', 'insuline', 'fonds', \"d'imiter\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def find_analogy(word1, word2, word3, word_vectors_dict, input_words, num_results=5):\n",
    "    # Get the word vectors for the input words\n",
    "    word_vector1 = word_vectors_dict.get(word1)\n",
    "    word_vector2 = word_vectors_dict.get(word2)\n",
    "    word_vector3 = word_vectors_dict.get(word3)\n",
    "\n",
    "    # Check if any of the input words are not in the vocabulary\n",
    "    if any(vector is None for vector in [word_vector1, word_vector2, word_vector3]):\n",
    "        return None\n",
    "\n",
    "    # Calculate the result vector using vector arithmetic\n",
    "    result_vector = word_vector2 - word_vector1 + word_vector3\n",
    "\n",
    "    # Find closest words to the result vector (excluding input words)\n",
    "    closest_words = find_closest_words_excluding_input(result_vector, word_vectors_dict, input_words, num_results)\n",
    "\n",
    "    return closest_words\n",
    "\n",
    "# Example usage:\n",
    "input_words = ['paris', 'france', 'rome']\n",
    "result = find_analogy('paris', 'france', 'rome', word_vectors_dict, input_words, num_results=5)\n",
    "\n",
    "if result is not None:\n",
    "    print(f\"Words: {result}\")\n",
    "else:\n",
    "    print(\"One or more input words not found in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "04c61c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Print the closest words (excluding input words)\n",
    "print(\"vieux\" in all_words_fr)\n",
    "print(\"jeune\" in all_words_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a7550f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'jeune': ['italie', 'paris', 'awareness', \"d'information.\", 'japon;']\n"
     ]
    }
   ],
   "source": [
    "petit_vector = word_vectors_dict['france']\n",
    "grand_vector = word_vectors_dict['paris']\n",
    "vieux_vector = word_vectors_dict['italie']\n",
    "\n",
    "# Calculate the word vector for 'jeune' (young) as 'grand - petit + vieux'\n",
    "jeune_vector = grand_vector - petit_vector + vieux_vector\n",
    "\n",
    "# Find closest words to the 'jeune' vector\n",
    "closest_words_jeune = find_closest_words_excluding_input(jeune_vector, word_vectors_dict, ['petit', 'grand', 'vieux'], num_results=5)\n",
    "\n",
    "# Print the closest words to 'jeune'\n",
    "print(f\"Words similar to 'jeune': {closest_words_jeune}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "62f42c20",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_languages' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m input_dim \u001b[38;5;241m=\u001b[39m embedding_dim  \u001b[38;5;66;03m# Adjust based on your word embedding dimension\u001b[39;00m\n\u001b[1;32m     37\u001b[0m hidden_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m  \u001b[38;5;66;03m# Adjust based on your architecture\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m output_dim \u001b[38;5;241m=\u001b[39m num_languages  \u001b[38;5;66;03m# Adjust based on the number of languages\u001b[39;00m\n\u001b[1;32m     39\u001b[0m model \u001b[38;5;241m=\u001b[39m LanguageDetectionModel(input_dim, hidden_dim, output_dim)\n\u001b[1;32m     40\u001b[0m train_language_detection_model(model, train_loader, criterion, optimizer, num_epochs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_languages' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define your language detection model\n",
    "class LanguageDetectionModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LanguageDetectionModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.embedding(input)\n",
    "        output = self.fc(embedded.mean(dim=1))  # Example: Average word embeddings\n",
    "        return self.softmax(output)\n",
    "\n",
    "# Define your training loop\n",
    "def train_language_detection_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Load your word embeddings and prepare data\n",
    "# (Assuming you have word embeddings in 'word_vectors_dict', train_loader, criterion, optimizer, and labels)\n",
    "\n",
    "# Initialize and train the language detection model\n",
    "input_dim = embedding_dim  # Adjust based on your word embedding dimension\n",
    "hidden_dim = 256  # Adjust based on your architecture\n",
    "output_dim = num_languages  # Adjust based on the number of languages\n",
    "model = LanguageDetectionModel(input_dim, hidden_dim, output_dim)\n",
    "train_language_detection_model(model, train_loader, criterion, optimizer, num_epochs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
