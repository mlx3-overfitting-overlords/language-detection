{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf9a0361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "def get_wordlist(lang, column=None):\n",
    "    if column is None:  # set default value for column\n",
    "        column = lang\n",
    "    \n",
    "    dataset_path = f'./datasets/CL_{lang}-en.parquet'\n",
    "    df = pd.read_parquet(dataset_path)\n",
    "    \n",
    "    # Preprocess the text\n",
    "    wordlist = df[column].str.lower().str.split().explode().tolist()\n",
    "\n",
    "    return wordlist\n",
    "\n",
    "\n",
    "words_fr = get_wordlist('fr')\n",
    "words_en = get_wordlist('fr','en')\n",
    "words_es = get_wordlist('es')\n",
    "words_de = get_wordlist('de')\n",
    "words_it = get_wordlist('it')\n",
    "\n",
    "import itertools\n",
    "\n",
    "# words_all = list(itertools.chain(words_fr, words_en))\n",
    "words_all = list(itertools.chain(words_fr,words_it,words_de,words_es,words_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3792f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['empresas', 'e', 'individuos', 'se', 'comportan', 'y', 'actúan', 'entre', 'sí.', 'giulio']\n"
     ]
    }
   ],
   "source": [
    "print(words_all[9000010:9000020])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10a4cb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Word2Vec Dataset Class\n",
    "class W2VData(torch.utils.data.Dataset):\n",
    "    def __init__(self, wordlist, window_size=2):\n",
    "        # Initialize a tokenizer with the provided corpus\n",
    "        self.wordlist = wordlist\n",
    "        self.data = []\n",
    "        # Create training samples using the window size provided\n",
    "        self.create_tuples(window_size)\n",
    "\n",
    "    def create_tuples(self, window_size):\n",
    "        # Create context and target pairs for training word2vec\n",
    "        for i, target in enumerate(tokens):\n",
    "            context = tokens[max(0, i - window_size):i] + tokens[i + 1:i + window_size + 1]\n",
    "            # Only consider cases where context is of the defined window size\n",
    "            if len(context) != 2 * window_size: continue\n",
    "            self.data.append((context, target))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Fetch context and target pairs by index\n",
    "        context, target = self.data[idx]\n",
    "        return torch.tensor(context), torch.tensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fed3166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SkipGram Model\n",
    "class SkipGram(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGram, self).__init__()\n",
    "        # Define embeddings layer: maps word indices to embedding vectors\n",
    "        self.embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Linear layer: maps a single embedding to vocabulary size (used for predicting the context words)\n",
    "        self.linear = torch.nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, target_word):\n",
    "        # Get the embedding of the target word\n",
    "        embeds = self.embeddings(target_word)\n",
    "        # Pass the embedding through the linear layer\n",
    "        out = self.linear(embeds)\n",
    "        # Apply log softmax to get log probabilities for predicting the context words\n",
    "        log_probs = torch.nn.functional.log_softmax(out, dim=1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423c9008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
