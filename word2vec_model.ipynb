{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ada9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4246e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data - you should replace this with your own dataset\n",
    "corpus = [\n",
    "    \"i like deep learning\",\n",
    "    \"deep learning is fun\",\n",
    "    \"machine learning is interesting\",\n",
    "]\n",
    "\n",
    "# Tokenize the corpus\n",
    "corpus = [sentence.split() for sentence in corpus]\n",
    "\n",
    "# Create a vocabulary\n",
    "vocab = set(word for sentence in corpus for word in sentence)\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ab739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBOW context window size\n",
    "context_size = 2\n",
    "\n",
    "# Create training data\n",
    "data = []\n",
    "for sentence in corpus:\n",
    "    for i in range(context_size, len(sentence) - context_size):\n",
    "        context = [sentence[i - j] for j in range(context_size)] + [sentence[i + j] for j in range(1, context_size + 1)]\n",
    "        target = sentence[i]\n",
    "        data.append((context, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59db8795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a CBOW dataset\n",
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, data, word_to_idx, context_size):\n",
    "        self.data = data\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.context_size = context_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.data[idx]\n",
    "        context_indices = [self.word_to_idx[word] for word in context]\n",
    "        target_index = self.word_to_idx[target]\n",
    "        return (context_indices, target_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f36ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader\n",
    "dataset = CBOWDataset(data, word_to_idx, context_size)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Define the CBOW model\n",
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOWModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, context):\n",
    "        embedded = self.embeddings(context).sum(dim=0)\n",
    "        scores = self.linear(embedded)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4de20f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 100\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "\n",
    "# Create and train the CBOW model\n",
    "model = CBOWModel(vocab_size, embedding_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for context_indices, target_index in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        context_indices = context_indices[0]\n",
    "        target_index = target_index[0]\n",
    "        scores = model(context_indices)\n",
    "        loss = criterion(scores, target_index)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(dataloader)}')\n",
    "\n",
    "# Retrieve word embeddings from the model\n",
    "word_embeddings = model.embeddings.weight.data.numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
